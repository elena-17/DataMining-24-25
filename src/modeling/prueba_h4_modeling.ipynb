{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "GOLD_DATA_PATH = os.path.join(\"..\", \"..\", \"data/gold/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(GOLD_DATA_PATH + \"data_card_4_df.csv\", sep=\";\", encoding = 'latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.drop(columns=[\"Unnamed: 0\"])\n",
    "df4.set_index(\"Provincias\", inplace=True)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df4.columns\n",
    "crr_results = []\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i + 1, len(columns)):\n",
    "        col1 = columns[i]\n",
    "        col2 = columns[j]\n",
    "\n",
    "        # Calculate Pearson correlation\n",
    "        pearson = scipy.stats.pearsonr(df4[col1], df4[col2])\n",
    "        pearson_corr = pearson[0]\n",
    "        pearson_pval = pearson[1]\n",
    "        \n",
    "        # Calculate Spearman correlation\n",
    "        spear = scipy.stats.spearmanr(df4[col1], df4[col2])\n",
    "        spear_corr = spear.correlation\n",
    "        spear_pval = spear.pvalue\n",
    "\n",
    "            \n",
    "        # Save results\n",
    "        crr_results.append({\n",
    "            'Feature 1': col1,\n",
    "            'Feature 2': col2,\n",
    "            'Pearson Correlation': pearson_corr,\n",
    "            'Pearson p-value': pearson_pval,\n",
    "            'Spearman Correlation': spear_corr,\n",
    "            'Spearman p-value': spear_pval\n",
    "        })\n",
    "\n",
    "\n",
    "crr_results_df = pd.DataFrame(crr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df4)\n",
    "df4_scaled = pd.DataFrame(scaled_data, columns=df4.columns)\n",
    "\n",
    "estimator = PCA(n_components=2)\n",
    "X_pca = estimator.fit_transform(scaled_data)\n",
    "print(estimator.explained_variance_ratio_)\n",
    "pd.DataFrame(np.matrix.transpose(estimator.components_), index=df4.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_pca[:,0], X_pca[:,1], s=50)\n",
    "\n",
    "# anotación \n",
    "for i in range(0, len(X_pca)):\n",
    "    ax.annotate(df4.iloc[i, :].name, (X_pca[i, 0], X_pca[i, 1]), fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "estimator = PCA(n_components=3)\n",
    "X_pca = estimator.fit_transform(scaled_data)\n",
    "print(estimator.explained_variance_ratio_)\n",
    "pd.DataFrame(np.matrix.transpose(estimator.components_), index=df4.columns)\n",
    "\n",
    "fig = px.scatter_3d(df4, x=X_pca[:,0], y=X_pca[:,1], z=X_pca[:,2])\n",
    "# anotación \n",
    "# for i in range(0, len(X_pca)):\n",
    "#     ax.text(X_pca[i, 0], X_pca[i, 1], X_pca[i, 2], df4.iloc[i, :].name, fontsize=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodo ward, minimiza la varianza intra-cluster\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "link_matrix_avg = linkage(scaled_data, method='ward', metric='euclidean')\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(link_matrix_avg, labels=df4.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodo single, minimiza la distancia entre los puntos mas cercanos\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "link_matrix = linkage(scaled_data, method='single', metric='euclidean')\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(link_matrix, labels=df4.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodo complete, minimiza la distancia entre los puntos mas lejanos\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "link_matrix = linkage(scaled_data, method='complete', metric='euclidean')\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(link_matrix, labels=df4.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodo average, minimiza la distancia promedio entre los puntos\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "link_matrix = linkage(scaled_data, method='average', metric='euclidean')\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(link_matrix, labels=df4.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "import numpy as np\n",
    "\n",
    "# Realizar el clustering jerárquico (usando la matriz de enlace creada previamente)\n",
    "# 'linkage_matrix' debe estar calculada con el método 'ward' u otro método.\n",
    "# # Paso 1: Generar muchos clusters inicialmente (granularidad alta)\n",
    "# clusters_granular = fcluster(link_matrix_avg, t=10, criterion='maxclust')  # Generar 10 clusters\n",
    "\n",
    "# # Agregar los clusters al DataFrame original para observar los resultados\n",
    "# df4[\"Cluster_Granular\"] = clusters_granular\n",
    "# df4_reset = df4.reset_index()  # Reset the index to access \"Provincias\" column\n",
    "# print(\"Clusters iniciales con granularidad alta:\")\n",
    "# print(df4_reset[[\"Provincias\", \"Cluster_Granular\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paso 2: Reagrupar o inspeccionar los clusters manualmente\n",
    "# # Aquí observamos los tamaños de los clusters\n",
    "# cluster_sizes = df4[\"Cluster_Granular\"].value_counts()\n",
    "# print(\"\\nTamaño de cada cluster inicial:\")\n",
    "# print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette: cuantifica la cohesión y la separación de los grupos. Valores cercanos a 1 indican que los puntos están bien agrupados y los grupos están separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(2, 10):\n",
    "    clusters = fcluster(link_matrix_avg, t=i, criterion='maxclust')  # Generar i clusters\n",
    "    scatter = axes[i-2].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, s=50, cmap='rainbow')\n",
    "    coef = metrics.silhouette_score(df4_scaled, clusters)\n",
    "    axes[i-2].legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "    axes[i-2].set_title(f'{i} Clusters score: {coef:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Cortar a un nivel lógico, por ejemplo, para obtener 5 clusters\n",
    "clusters_final = fcluster(link_matrix_avg, t=3, criterion='maxclust')  # Cortar a 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tratamos el cluster -1 como cluster de outliers\n",
    "from sklearn import metrics\n",
    "n_clusters_ = len(set(clusters_final)) - (1 if -1 in clusters_final else 0)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(df4, clusters_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"Cluster\"] = clusters_final\n",
    "df4_scaled[\"Cluster\"] = clusters_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting orginal points with color related to label\n",
    "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters_final, s=50)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(df4, x=X_pca[:,0], y=X_pca[:,1], z=X_pca[:,2], color=clusters_final)\n",
    "# anotación \n",
    "# for i in range(0, len(X_pca)):\n",
    "#     ax.text(X_pca[i, 0], X_pca[i, 1], X_pca[i, 2], df4.iloc[i, :].name, fontsize=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plotear las variables estandarizadas por cluster\n",
    "df4_melted = df4_scaled.melt(id_vars=\"Cluster\", var_name=\"Variable\", value_name=\"Valor\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Variable\", y=\"Valor\", hue=\"Cluster\", data=df4_melted, palette=\"Set3\")\n",
    "plt.title(\"Distribución de las variables por cluster (estandarizadas)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[[\"Cluster\"]].to_csv(\"clusters_mapa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/Edad_mediana_2C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/Edad_mediana_3C.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
