{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook es una modificación del notebook de `h4_modeling` pero cambiando la variable de edad usando la mediana a la media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "GOLD_DATA_PATH = os.path.join(\"..\", \"..\", \"data/gold/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Los datos tendrá como base la tarjeta de datos 4 (`data_card_4_df.csv`), de la que se eliminará la variable edad para añadir la variable de `Edad media` de la tarjeta de datos 1 (`data_card_1_df.csv`).\n",
    "\n",
    "La mediana es una medida más representativa que la media, al no estar influida por valores extremos. Sin embargo, en este caso, la forma de calcular la mediana en el paso de preprocesamiento, hace que la variable de edad sea una variable categórica. Por esto, se ha decidido utilizar la media y comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar los datos\n",
    "df1 = pd.read_csv(GOLD_DATA_PATH + \"data_card_1_df.csv\", sep=\";\", encoding = 'latin')\n",
    "\n",
    "df4 = pd.read_csv(GOLD_DATA_PATH + \"data_card_4_df.csv\", sep=\";\", encoding = 'latin')\n",
    "df4.drop(columns=[\"Unnamed: 0\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores de la columna de edad: \",df4['Mediana edad'].unique())\n",
    "df4.drop(columns=[\"Mediana edad\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df4, df1['Edad media']], axis=1)\n",
    "df4.set_index('Provincias', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de los datos a un rango de 0 a 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df4)\n",
    "df4_scaled = pd.DataFrame(scaled_data, columns=df4.columns, index=df4.index)\n",
    "\n",
    "df4_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de componentes principales\n",
    "\n",
    "Para visualizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimator = PCA(n_components=3)\n",
    "X_pca = estimator.fit_transform(scaled_data)\n",
    "\n",
    "print(\"Porcentaje de varianza explicado por cada componente:\\n\", estimator.explained_variance_ratio_)\n",
    "pd.DataFrame(np.matrix.transpose(estimator.components_), index=df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representación 2D\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_pca[:,0], X_pca[:,1], s=50)\n",
    "\n",
    "# anotación \n",
    "for i in range(0, len(X_pca)):\n",
    "    ax.annotate(df4.iloc[i, :].name, (X_pca[i, 0], X_pca[i, 1]), fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se compara este gráfico con el que se obtiene con la mediana, es una versión espejada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodo ward, minimiza la varianza intra-cluster\n",
    "link_matrix_avg = linkage(scaled_data, method='ward', metric='euclidean')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(link_matrix_avg, labels=df4.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de diferente número de clusters\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(2, 10):\n",
    "    clusters = fcluster(link_matrix_avg, t=i, criterion='maxclust')  # Generar i clusters\n",
    "    scatter = axes[i-2].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, s=50, cmap='rainbow')\n",
    "    coef = silhouette_score(df4_scaled, clusters)\n",
    "    axes[i-2].legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "    axes[i-2].set_title(f'{i} Clusters score: {coef:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionamos 2 clusters\n",
    "clusters = fcluster(link_matrix_avg, t=2, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualización del resultado\n",
    "plt.figure(figsize=(8, 5))\n",
    "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], s=50, c=clusters, cmap='plasma_r')\n",
    "\n",
    "# nombres de las provincias \n",
    "for i in range(0, len(X_pca)):\n",
    "    plt.annotate(df4.iloc[i, :].name, (X_pca[i, 0], X_pca[i, 1]), fontsize=8)\n",
    "\n",
    "handles, labels = scatter.legend_elements()\n",
    "legend = plt.legend(handles, labels, title=\"Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Clusters:\n",
    "\n",
    "![img](img/Edad_media_2C.png)\n",
    "\n",
    "3 Clusters:\n",
    "\n",
    "![img](img/Edad_media_3C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación y clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar los resultados y sacar conclusiones, se realizará una división de las provincias en norte y sur (criterio?) y se etiquetarán los grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provincias en el dataset\n",
    "df4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norte=['Araba/Álava', 'Asturias', 'Barcelona', 'Bizkaia', 'Burgos', 'Cantabria', 'Coruña, A', 'Gipuzkoa', 'Girona', 'Guadalajara', 'Huesca', 'León', 'Lleida', 'Lugo', 'Navarra', 'Ourense', 'Palencia', 'Pontevedra', 'Rioja, La', 'Salamanca', 'Segovia', 'Soria', 'Tarragona', 'Teruel', 'Valladolid', 'Zamora', 'Zaragoza', 'Ávila']\n",
    "\n",
    "df4['Grupo'] = [0 if x in norte else 1 for x in df4.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Grupo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y validación\n",
    "\n",
    "Se entrenará un modelo de clasificación, se ha elegido Random Forest, y se sacará el valor de importancia de cada variable para volver a clusterizar y comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df4.drop(columns=[\"Grupo\"])\n",
    "y = df4[\"Grupo\"]\n",
    "\n",
    "# se divide el dataset en 30-70 para que el modelo no entrene con demasiados datos y evitar el overfitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de resultados\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "cm_display.plot()\n",
    "plt.title(\"Confussion Matrix\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo muestra buenos resultados durante el entrenamiento. Pero al tener muy pocos datos, se comprobarán los resultados usando validación cruzada del tipo Leave One Out. Este tipo de validación es muy costoso computacionalmente, pero al tener pocos datos, es la mejor opción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "scores = cross_val_score(rf, X, y, cv=LeaveOneOut())\n",
    "print(f\"Accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Feature Relevances')\n",
    "print(pd.DataFrame({'Attributes': X_train.columns,\n",
    "            'Feature importance':rf.feature_importances_}).sort_values('Feature importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = df4_scaled[['Producción media', 'Renta']]\n",
    "\n",
    "link_matrix_avg = linkage(selection, method='ward', metric='euclidean')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(link_matrix_avg, labels=df4.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = fcluster(link_matrix_avg, t=2, criterion='maxclust')\n",
    "df4['Cluster'] = clusters\n",
    "\n",
    "r = df4[['Grupo', 'Cluster']]\n",
    "r.to_csv('cluster.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del clustering jerárquico después de realizar la selección de características:\n",
    "\n",
    "![img](img/seleccion.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
